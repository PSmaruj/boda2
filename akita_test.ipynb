{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import boda\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/home1/smaruj/AkitaMini-pytorch\")  # Add the directory where \"ledidi\" is located\n",
    "from model import SeqNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from akita_helper import plot_map, from_upper_triu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/SLURM_209730/ipykernel_2031170/2425661507.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/home1/smaruj/AkitaMini-pytorch/best_model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SeqNN()\n",
    "\n",
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load('/home1/smaruj/AkitaMini-pytorch/best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeqNN(\n",
       "  (stochastic_reverse_complement): StochasticReverseComplement()\n",
       "  (stochastic_shift): StochasticShift()\n",
       "  (re_lu): ReLU()\n",
       "  (conv_block_1): ConvBlock(\n",
       "    (conv): Conv1d(4, 96, kernel_size=(11,), stride=(1,), padding=(5,), bias=False)\n",
       "    (batch_norm): BatchNorm1d(96, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "    (pool): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_tower): ConvTower(\n",
       "    (conv_tower): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): Conv1d(96, 96, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "      (2): BatchNorm1d(96, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "      (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): ReLU()\n",
       "      (5): Conv1d(96, 96, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "      (6): BatchNorm1d(96, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "      (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): ReLU()\n",
       "      (9): Conv1d(96, 96, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "      (10): BatchNorm1d(96, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "      (11): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (12): ReLU()\n",
       "      (13): Conv1d(96, 96, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "      (14): BatchNorm1d(96, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "      (15): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (16): ReLU()\n",
       "      (17): Conv1d(96, 96, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "      (18): BatchNorm1d(96, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "      (19): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (20): ReLU()\n",
       "      (21): Conv1d(96, 96, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "      (22): BatchNorm1d(96, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "      (23): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (residual1d_block1): ResidualDilatedBlock1D(\n",
       "    (relu1): ReLU()\n",
       "    (conv1): Conv1d(96, 48, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    (bn1): BatchNorm1d(48, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU()\n",
       "    (conv2): Conv1d(48, 96, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (bn2): BatchNorm1d(96, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       "  (conv_reduce): ConvBlockReduce(\n",
       "    (layers): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): Conv1d(96, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "      (2): BatchNorm1d(64, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (one_to_two): OneToTwo()\n",
       "  (concat_dist): ConcatDist2D()\n",
       "  (conv2d_block): Conv2DBlock(\n",
       "    (block): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(48, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (symmetrize_2d): Symmetrize2D()\n",
       "  (residual2d_block1): DilatedResidualBlock2D(\n",
       "    (relu): ReLU()\n",
       "    (conv1): Conv2d(48, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(24, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(48, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
       "    (symmetrize): Symmetrize2D()\n",
       "  )\n",
       "  (residual2d_block2): DilatedResidualBlock2D(\n",
       "    (relu): ReLU()\n",
       "    (conv1): Conv2d(48, 24, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "    (bn1): BatchNorm2d(24, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False)\n",
       "    (bn2): BatchNorm2d(48, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
       "    (symmetrize): Symmetrize2D()\n",
       "  )\n",
       "  (residual2d_block3): DilatedResidualBlock2D(\n",
       "    (relu): ReLU()\n",
       "    (conv1): Conv2d(48, 24, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "    (bn1): BatchNorm2d(24, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), dilation=(4, 4), bias=False)\n",
       "    (bn2): BatchNorm2d(48, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
       "    (symmetrize): Symmetrize2D()\n",
       "  )\n",
       "  (residual2d_block4): DilatedResidualBlock2D(\n",
       "    (relu): ReLU()\n",
       "    (conv1): Conv2d(48, 24, kernel_size=(3, 3), stride=(1, 1), padding=(7, 7), dilation=(7, 7), bias=False)\n",
       "    (bn1): BatchNorm2d(24, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), dilation=(7, 7), bias=False)\n",
       "    (bn2): BatchNorm2d(48, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
       "    (symmetrize): Symmetrize2D()\n",
       "  )\n",
       "  (cropping_2d): Cropping2D()\n",
       "  (upper_tri): UpperTri()\n",
       "  (final): Final(\n",
       "    (dense): Linear(in_features=48, out_features=1, bias=True)\n",
       "  )\n",
       "  (switch_reverse_triu): SwitchReverseTriu()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model to evaluation mode (important for inference)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "SeqNN                                    [2, 1, 1953]              --\n",
       "├─StochasticReverseComplement: 1-1       [2, 4, 32768]             --\n",
       "├─StochasticShift: 1-2                   [2, 4, 32768]             --\n",
       "├─ReLU: 1-3                              [2, 4, 32768]             --\n",
       "├─ConvBlock: 1-4                         [2, 96, 4096]             --\n",
       "│    └─Conv1d: 2-1                       [2, 96, 32768]            4,224\n",
       "│    └─BatchNorm1d: 2-2                  [2, 96, 32768]            192\n",
       "│    └─MaxPool1d: 2-3                    [2, 96, 4096]             --\n",
       "├─ConvTower: 1-5                         [2, 96, 64]               --\n",
       "│    └─Sequential: 2-4                   [2, 96, 64]               --\n",
       "│    │    └─ReLU: 3-1                    [2, 96, 4096]             --\n",
       "│    │    └─Conv1d: 3-2                  [2, 96, 4096]             46,080\n",
       "│    │    └─BatchNorm1d: 3-3             [2, 96, 4096]             192\n",
       "│    │    └─MaxPool1d: 3-4               [2, 96, 2048]             --\n",
       "│    │    └─ReLU: 3-5                    [2, 96, 2048]             --\n",
       "│    │    └─Conv1d: 3-6                  [2, 96, 2048]             46,080\n",
       "│    │    └─BatchNorm1d: 3-7             [2, 96, 2048]             192\n",
       "│    │    └─MaxPool1d: 3-8               [2, 96, 1024]             --\n",
       "│    │    └─ReLU: 3-9                    [2, 96, 1024]             --\n",
       "│    │    └─Conv1d: 3-10                 [2, 96, 1024]             46,080\n",
       "│    │    └─BatchNorm1d: 3-11            [2, 96, 1024]             192\n",
       "│    │    └─MaxPool1d: 3-12              [2, 96, 512]              --\n",
       "│    │    └─ReLU: 3-13                   [2, 96, 512]              --\n",
       "│    │    └─Conv1d: 3-14                 [2, 96, 512]              46,080\n",
       "│    │    └─BatchNorm1d: 3-15            [2, 96, 512]              192\n",
       "│    │    └─MaxPool1d: 3-16              [2, 96, 256]              --\n",
       "│    │    └─ReLU: 3-17                   [2, 96, 256]              --\n",
       "│    │    └─Conv1d: 3-18                 [2, 96, 256]              46,080\n",
       "│    │    └─BatchNorm1d: 3-19            [2, 96, 256]              192\n",
       "│    │    └─MaxPool1d: 3-20              [2, 96, 128]              --\n",
       "│    │    └─ReLU: 3-21                   [2, 96, 128]              --\n",
       "│    │    └─Conv1d: 3-22                 [2, 96, 128]              46,080\n",
       "│    │    └─BatchNorm1d: 3-23            [2, 96, 128]              192\n",
       "│    │    └─MaxPool1d: 3-24              [2, 96, 64]               --\n",
       "├─ResidualDilatedBlock1D: 1-6            [2, 96, 64]               --\n",
       "│    └─ReLU: 2-5                         [2, 96, 64]               --\n",
       "│    └─Conv1d: 2-6                       [2, 48, 64]               13,824\n",
       "│    └─BatchNorm1d: 2-7                  [2, 48, 64]               96\n",
       "│    └─ReLU: 2-8                         [2, 48, 64]               --\n",
       "│    └─Conv1d: 2-9                       [2, 96, 64]               4,608\n",
       "│    └─BatchNorm1d: 2-10                 [2, 96, 64]               192\n",
       "│    └─Dropout: 2-11                     [2, 96, 64]               --\n",
       "├─ConvBlockReduce: 1-7                   [2, 64, 64]               --\n",
       "│    └─Sequential: 2-12                  [2, 64, 64]               --\n",
       "│    │    └─ReLU: 3-25                   [2, 96, 64]               --\n",
       "│    │    └─Conv1d: 3-26                 [2, 64, 64]               30,720\n",
       "│    │    └─BatchNorm1d: 3-27            [2, 64, 64]               128\n",
       "│    │    └─ReLU: 3-28                   [2, 64, 64]               --\n",
       "├─OneToTwo: 1-8                          [2, 64, 64, 64]           --\n",
       "├─ConcatDist2D: 1-9                      [2, 65, 64, 64]           --\n",
       "├─Conv2DBlock: 1-10                      [2, 48, 64, 64]           --\n",
       "│    └─Sequential: 2-13                  [2, 48, 64, 64]           --\n",
       "│    │    └─ReLU: 3-29                   [2, 65, 64, 64]           --\n",
       "│    │    └─Conv2d: 3-30                 [2, 48, 64, 64]           28,080\n",
       "│    │    └─BatchNorm2d: 3-31            [2, 48, 64, 64]           96\n",
       "├─Symmetrize2D: 1-11                     [2, 48, 64, 64]           --\n",
       "├─DilatedResidualBlock2D: 1-12           [2, 48, 64, 64]           --\n",
       "│    └─ReLU: 2-14                        [2, 48, 64, 64]           --\n",
       "│    └─Conv2d: 2-15                      [2, 24, 64, 64]           10,368\n",
       "│    └─BatchNorm2d: 2-16                 [2, 24, 64, 64]           48\n",
       "│    └─ReLU: 2-17                        [2, 24, 64, 64]           --\n",
       "│    └─Conv2d: 2-18                      [2, 48, 64, 64]           1,152\n",
       "│    └─BatchNorm2d: 2-19                 [2, 48, 64, 64]           96\n",
       "│    └─Dropout2d: 2-20                   [2, 48, 64, 64]           --\n",
       "│    └─Symmetrize2D: 2-21                [2, 48, 64, 64]           --\n",
       "├─DilatedResidualBlock2D: 1-13           [2, 48, 64, 64]           --\n",
       "│    └─ReLU: 2-22                        [2, 48, 64, 64]           --\n",
       "│    └─Conv2d: 2-23                      [2, 24, 64, 64]           10,368\n",
       "│    └─BatchNorm2d: 2-24                 [2, 24, 64, 64]           48\n",
       "│    └─ReLU: 2-25                        [2, 24, 64, 64]           --\n",
       "│    └─Conv2d: 2-26                      [2, 48, 64, 64]           1,152\n",
       "│    └─BatchNorm2d: 2-27                 [2, 48, 64, 64]           96\n",
       "│    └─Dropout2d: 2-28                   [2, 48, 64, 64]           --\n",
       "│    └─Symmetrize2D: 2-29                [2, 48, 64, 64]           --\n",
       "├─DilatedResidualBlock2D: 1-14           [2, 48, 64, 64]           --\n",
       "│    └─ReLU: 2-30                        [2, 48, 64, 64]           --\n",
       "│    └─Conv2d: 2-31                      [2, 24, 64, 64]           10,368\n",
       "│    └─BatchNorm2d: 2-32                 [2, 24, 64, 64]           48\n",
       "│    └─ReLU: 2-33                        [2, 24, 64, 64]           --\n",
       "│    └─Conv2d: 2-34                      [2, 48, 64, 64]           1,152\n",
       "│    └─BatchNorm2d: 2-35                 [2, 48, 64, 64]           96\n",
       "│    └─Dropout2d: 2-36                   [2, 48, 64, 64]           --\n",
       "│    └─Symmetrize2D: 2-37                [2, 48, 64, 64]           --\n",
       "├─DilatedResidualBlock2D: 1-15           [2, 48, 64, 64]           --\n",
       "│    └─ReLU: 2-38                        [2, 48, 64, 64]           --\n",
       "│    └─Conv2d: 2-39                      [2, 24, 64, 64]           10,368\n",
       "│    └─BatchNorm2d: 2-40                 [2, 24, 64, 64]           48\n",
       "│    └─ReLU: 2-41                        [2, 24, 64, 64]           --\n",
       "│    └─Conv2d: 2-42                      [2, 48, 64, 64]           1,152\n",
       "│    └─BatchNorm2d: 2-43                 [2, 48, 64, 64]           96\n",
       "│    └─Dropout2d: 2-44                   [2, 48, 64, 64]           --\n",
       "│    └─Symmetrize2D: 2-45                [2, 48, 64, 64]           --\n",
       "├─Cropping2D: 1-16                       [2, 48, 64, 64]           --\n",
       "├─UpperTri: 1-17                         [2, 48, 1953]             --\n",
       "├─Final: 1-18                            [2, 1, 1953]              --\n",
       "│    └─Linear: 2-46                      [2, 1953, 1]              49\n",
       "├─SwitchReverseTriu: 1-19                [2, 1, 1953]              --\n",
       "==========================================================================================\n",
       "Total params: 406,497\n",
       "Trainable params: 406,497\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.63\n",
       "==========================================================================================\n",
       "Input size (MB): 1.05\n",
       "Forward/backward pass size (MB): 169.93\n",
       "Params size (MB): 1.63\n",
       "Estimated Total Size (MB): 172.61\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(2, 4, 32768), col_names=[\"output_size\", \"num_params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/SLURM_209730/ipykernel_2031170/413687493.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_data = torch.load('/home1/smaruj/AkitaMini-pytorch/test_data.pt')\n"
     ]
    }
   ],
   "source": [
    "# Load the test data\n",
    "test_data = torch.load('/home1/smaruj/AkitaMini-pytorch/test_data.pt')\n",
    "\n",
    "# Assume test_data is a tuple of (X, y)\n",
    "X_test, y_test = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 3\n",
    "X_batch = X_test[:num_examples]\n",
    "y_batch = y_test[:num_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch, y_batch = X_batch.to(device), y_batch.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 1953])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to get the indices of the upper triangular part for a given matrix size\n",
    "def get_upper_triu_indices(dim, num_diags=2):\n",
    "    return np.triu_indices(dim, k=num_diags)\n",
    "\n",
    "# The matrix size you are working with\n",
    "dim = 64\n",
    "# The starting and ending indices for the chunk (0:21, 21:38)\n",
    "start_row, end_row = 0, 21\n",
    "start_col, end_col = 21, 38\n",
    "\n",
    "# Get the full upper triangular indices for the 64x64 matrix\n",
    "full_indices = get_upper_triu_indices(dim)\n",
    "\n",
    "# Now create a mask to extract the relevant slice of the matrix\n",
    "mask = ((full_indices[0] >= start_row) & (full_indices[0] < end_row) &\n",
    "        (full_indices[1] >= start_col) & (full_indices[1] < end_col))\n",
    "\n",
    "# Extract the corresponding indices from the full 64x64 vector\n",
    "sub_indices_in_full_vector = np.where(mask)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a list of indices into a list of slices\n",
    "def indices_to_slices(indices):\n",
    "    slices = []\n",
    "    start = indices[0]\n",
    "    \n",
    "    for i in range(1, len(indices)):\n",
    "        if indices[i] != indices[i-1] + 1:  # Check if the indices are contiguous\n",
    "            slices.append(slice(start, indices[i-1] + 1))\n",
    "            start = indices[i]\n",
    "    \n",
    "    # Append the last slice, without including step=None\n",
    "    slices.append(slice(start, indices[-1] + 1))\n",
    "    \n",
    "    # Remove the step argument from the slices (if it's None)\n",
    "    return [slice(s.start, s.stop) for s in slices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = indices_to_slices(sub_indices_in_full_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[slice(19, 36, None),\n",
       " slice(80, 97, None),\n",
       " slice(140, 157, None),\n",
       " slice(199, 216, None),\n",
       " slice(257, 274, None),\n",
       " slice(314, 331, None),\n",
       " slice(370, 387, None),\n",
       " slice(425, 442, None),\n",
       " slice(479, 496, None),\n",
       " slice(532, 549, None),\n",
       " slice(584, 601, None),\n",
       " slice(635, 652, None),\n",
       " slice(685, 702, None),\n",
       " slice(734, 751, None),\n",
       " slice(782, 799, None),\n",
       " slice(829, 846, None),\n",
       " slice(875, 892, None),\n",
       " slice(920, 937, None),\n",
       " slice(964, 981, None),\n",
       " slice(1007, 1024, None),\n",
       " slice(1050, 1066, None)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast SeqProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.randn(\n",
    "    1, 4, 336\n",
    ")\n",
    "\n",
    "left_flank = X_batch[1,:, :10328].unsqueeze(0)\n",
    "\n",
    "right_flank = X_batch[1,:, :-10664].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 10328]), torch.Size([1, 4, 22104]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_flank.shape, right_flank.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10328 + 22104 + 336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StraightThroughParameters(\n",
       "  (norm): InstanceNorm1d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = boda.generator.StraightThroughParameters(\n",
    "    data=logits, left_flank=left_flank, right_flank=right_flank, \n",
    "    n_samples=1, use_affine=False\n",
    ")\n",
    "params.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# energy = boda.generator.MinGapEnergy(model, target_feature=21, target_alpha=-1, a_min=-2., a_max=6.)\n",
    "# energy = boda.generator.MinGapEnergy(model, target_feature=slice(96,980), target_alpha=-1, a_min=-2., a_max=6.)\n",
    "energy = boda.generator.MinGapEnergy(model, target_feature=slice(96,980), target_alpha=-1, a_min=-2., a_max=6.)\n",
    "\n",
    "#Higher or Lower?:\n",
    "# - If you want the target feature to be higher in the energy calculation (i.e., the energy should be lower when the target feature is higher), \n",
    "# then you’ll use a positive target_alpha.\n",
    "# - If you want the target feature to be lower (i.e., the energy should be lower when the target feature is lower), \n",
    "# then you’ll use a negative target_alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FSP = boda.generator.FastSeqProp(energy, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FSP.run(n_steps=1000, lr_scheduler=True, create_plot=True)\n",
    "proposals = params.get_sample()\n",
    "proposals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezed_proposals = proposals.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezed_proposals.shape, logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_seq = X_batch[1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_seq[:, 10328:10664] = squeezed_proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_seq_dim = og_seq.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    predictions_new = model(og_seq_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_map(from_upper_triu(predictions[1], matrix_len=64, num_diags=2), vmin=-0.6, vmax=0.6, palette=\"RdBu_r\", width=5, height=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_map(from_upper_triu(predictions_new, matrix_len=64, num_diags=2), vmin=-0.6, vmax=0.6, palette=\"RdBu_r\", width=5, height=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_one = from_upper_triu(predictions[1], matrix_len=64, num_diags=2)\n",
    "map_two = from_upper_triu(predictions_new, matrix_len=64, num_diags=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(map_one), np.nanmean(map_two)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cuda11.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
